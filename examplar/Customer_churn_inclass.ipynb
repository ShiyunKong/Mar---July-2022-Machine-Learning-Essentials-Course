{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R88Ms0MTi0Ma"
   },
   "source": [
    "# User Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WA6lL1fni0Mb"
   },
   "source": [
    "In this project, we use supervised learning models to identify customers who are likely to stop using service in the future. Furthermore, we will analyze top factors that influence user retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUoI2S7Bi6iR"
   },
   "source": [
    "# Part 0: Read the data"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "id": "i0SsYDD5VQWF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "oq-dWT9pU7iT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# taking a look at the top 5 rows\n",
    "churn_df = pd.read_csv('/content/drive/MyDrive/GEC/project/churn.csv.all')\n",
    "churn_df.head()"
   ],
   "metadata": {
    "id": "qvKt1EgPVR5E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6bG_gAPi0Me"
   },
   "source": [
    "# Part 1: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bspx2K6fi0Me"
   },
   "source": [
    "### Part 1.1: Understand the Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuTHKjk-i0Mf"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C99Z9b7ai0Mm"
   },
   "outputs": [],
   "source": [
    "# find how many samples are there in the data\n",
    "# find how many variables are there in the data\n",
    "print (\"Num of rows: \" + str(churn_df.shape[0]))\n",
    "print (\"Num of columns: \" + str(churn_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bBCQcTlL-KQ"
   },
   "outputs": [],
   "source": [
    "# take a look at all variable name\n",
    "churn_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCglmJ9Oi0Mo"
   },
   "source": [
    "### Part 1.2: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Vf8iYmWi0Mq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking the categorical features\n",
    "churn_df['voice_mail_plan'][0]\n",
    "churn_df['intl_plan'][0]\n",
    "churn_df['churned'][0]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "np.sum(churn_df['churned']==' False.')"
   ],
   "metadata": {
    "id": "o1Kt2KICTMkC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lpwxvQfi0Mt"
   },
   "outputs": [],
   "source": [
    "# lambda function to remove the heading the trailing white space\n",
    "# apply(lambda x: function_of_x)\n",
    "churn_df['voice_mail_plan'] = churn_df['voice_mail_plan'].apply(lambda x: x.strip())\n",
    "churn_df['intl_plan'] = churn_df['intl_plan'].apply(lambda x: x.strip())\n",
    "churn_df['churned'] = churn_df['churned'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcyHhHKHZN2p"
   },
   "outputs": [],
   "source": [
    "churn_df['voice_mail_plan'][0]\n",
    "churn_df['intl_plan'][0]\n",
    "churn_df['churned'][0]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# check the number of case in different class\n",
    "# we always want to do this, because we want to know whether or not the data is balanced\n",
    "# and this will influence how well the model can get\n",
    "print(sum(churn_df['churned']=='True.'))\n",
    "print(sum(churn_df['churned']=='False.'))"
   ],
   "metadata": {
    "id": "YXHR786yCvnF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsAbAjhvi0Mx"
   },
   "source": [
    "### Part 1.3:  Understand the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJ0AdxwLi0Mz",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# we can plot the distribution of a variable\n",
    "# it is bell-shaped, almost normally distributed\n",
    "sns.distplot(churn_df['total_intl_charge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DKTTdB6i0M2"
   },
   "outputs": [],
   "source": [
    "# make a heatmap of the pairwise-correlation\n",
    "corr = churn_df[[\"account_length\", \"number_vmail_messages\", \"total_day_minutes\",\n",
    "                    \"total_day_calls\", \"total_day_charge\", \"total_eve_minutes\",\n",
    "                    \"total_eve_calls\", \"total_eve_charge\", \"total_night_minutes\",\n",
    "                    \"total_night_calls\", \"total_intl_minutes\", \"total_intl_calls\",\n",
    "                    \"total_intl_charge\"]].corr()\n",
    "\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qfEnNW_i0M5"
   },
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFa4d6t3i0NH"
   },
   "source": [
    "# Part 2: Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtjI61m6i0M8"
   },
   "outputs": [],
   "source": [
    "# if you want to see only one correlation\n",
    "from scipy.stats import pearsonr\n",
    "print(pearsonr(churn_df['total_day_minutes'], churn_df['number_vmail_messages'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general(especially in the linear regression case), we want to exclude one of the pair that the correlation is higher than 0.9. For the correlation that is less than 0.9 but relative high, we want to consider the interaction term."
   ],
   "metadata": {
    "id": "3XsqoI-0ek8X"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ec5r_Qdi0NL"
   },
   "outputs": [],
   "source": [
    "# we want to get ground truth data\n",
    "y = np.where(churn_df['churned'] == 'False.',0,1)\n",
    "\n",
    "# we want to drop the columns that is not a reason to the result\n",
    "to_drop = ['state','area_code','phone_number','churned']\n",
    "churn_feat_space = churn_df.drop(to_drop, axis=1)\n",
    "\n",
    "# there are some yes/no column, we want to convert them into True/False\n",
    "yes_no_cols = [\"intl_plan\",\"voice_mail_plan\"]\n",
    "churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'\n",
    "\n",
    "X = churn_feat_space"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# see what is the shape of the data\n",
    "X.shape, y.shape"
   ],
   "metadata": {
    "id": "5MQ9sQ0uzQis"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzCo_GC97rGd"
   },
   "outputs": [],
   "source": [
    "num = y.sum() / y.shape[0] * 100\n",
    "print('There are %s percent of y that has value 1'% num)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Practice 1: Can you add catogorical features, e.g. state, into your feature matrix? (one suggestion would be one_hot_encoding)"
   ],
   "metadata": {
    "id": "IRQJ6Ochmt3s"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "churn_df['intl_plan']=churn_df['intl_plan'].astype('category')\n",
    "churn_df['voice_mail_plan']=churn_df['voice_mail_plan'].astype('category')\n",
    "churn_df['intl_plan_new']=churn_df['intl_plan'].cat.codes\n",
    "churn_df['voice_mail_plan_new']=churn_df['voice_mail_plan'].cat.codes\n",
    "encoder=OneHotEncoder()\n",
    "enc_data=pd.DataFrame(encoder.fit_transform(churn_df[['Gen_new','Rem_new']]).toarray())\n",
    "New_df=churn_df.join(enc_data)\n",
    "print(New_df)"
   ],
   "metadata": {
    "id": "-bFktZnuWJ4y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3x9ySX_i0Nd"
   },
   "source": [
    "# Part 3: Model Training and Result Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77OjmSl9i0Nf"
   },
   "source": [
    "### Part 3.1: Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uay8Md5li0Nh"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print('training data has %d observation with %d features'% X_train.shape)\n",
    "print('test data has %d observation with %d features'% X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuPhtUkJi0NW"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4UTtCQTi0Nl"
   },
   "source": [
    "### Part 3.2: Model Training and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAhSxINLi0Nl"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier_logistic = LogisticRegression()\n",
    "\n",
    "classifier_KNN = KNeighborsClassifier()\n",
    "\n",
    "classifier_RF = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Av0IRSoBQ3pe"
   },
   "outputs": [],
   "source": [
    "classifier_logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiLuzUDJRBNi"
   },
   "outputs": [],
   "source": [
    "classifier_logistic.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "classifier_logistic.predict_proba(X_test)"
   ],
   "metadata": {
    "id": "z0F-vAo5aXrg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjMV04mKRJ30"
   },
   "outputs": [],
   "source": [
    "classifier_logistic.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OCgNSNri0Nn"
   },
   "outputs": [],
   "source": [
    "model_names = ['Logistic Regression','KNN','Random Forest']\n",
    "model_list = [classifier_logistic, classifier_KNN, classifier_RF]\n",
    "count = 0\n",
    "\n",
    "for classifier in model_list:\n",
    "    cv_score = model_selection.cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    print(cv_score)\n",
    "    print('Model accuracy of %s is: %.3f'%(model_names[count],cv_score.mean()))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWEbbJgpahSI"
   },
   "outputs": [],
   "source": [
    "cv_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Practice 2: Can you do prediction with SVM model?"
   ],
   "metadata": {
    "id": "NwpksUWkoQpo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel=\"linear\").fit(X_train, y_train)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "c_score = cross_val_score(svc, X_train,y_train, cv=6)\n",
    "print(c_score)"
   ],
   "metadata": {
    "id": "7XMGQ33Ga4JQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J-23z78i0Ns"
   },
   "source": [
    "### Part 3.3: Use Grid Search to Find Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hpe9PEAAi0Nt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def print_grid_search_metrics(gs):\n",
    "    print (\"Best score: %0.3f\" % gs.best_score_)\n",
    "    print (\"Best parameters set:\")\n",
    "    best_parameters = gs.best_params_\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvYo9I5Ti0Nv"
   },
   "source": [
    "#### Part 3.3.1: Find Optimal Hyperparameters - LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOc48syxi0Nx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'penalty':('l1', 'l2'), \n",
    "    'C':(1, 5, 10)\n",
    "}\n",
    "Grid_LR = GridSearchCV(LogisticRegression(),parameters, cv=5)\n",
    "Grid_LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nN5rU0e-i0N1"
   },
   "outputs": [],
   "source": [
    "print_grid_search_metrics(Grid_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtkDsXgui0N3"
   },
   "outputs": [],
   "source": [
    "best_LR_model = Grid_LR.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9u9YFedOi0N6"
   },
   "source": [
    "#### Part 3.3.2: Find Optimal Hyperparameters: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o78422XVi0N6"
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'n_neighbors':[3,5,7,10] \n",
    "}\n",
    "Grid_KNN = GridSearchCV(KNeighborsClassifier(),parameters, cv=5)\n",
    "Grid_KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydaRZVAIi0N_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_grid_search_metrics(Grid_KNN)\n",
    "best_KNN_model = Grid_KNN.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKn_oKLSi0OB"
   },
   "source": [
    "#### Part 3.3.3: \n",
    "#### Practice 3: Find Optimal Hyperparameters: Random Forest(using [40,60,80] as parameter space)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "PnlDk5OetQfU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxDAOrGIi0OI"
   },
   "source": [
    "### Part 3.4: Model Evaluation - Confusion Matrix (Precision, Recall, Accuracy)\n",
    "\n",
    "class of interest as positive\n",
    "\n",
    "TP: correctly labeled real churn\n",
    "\n",
    "Precision: tp / (tp + fp);\n",
    "Total number of true predictive churn divided by the total number of predictive churn;\n",
    "High Precision means low fp, not many return users were predicted as churn users. \n",
    "\n",
    "\n",
    "Recall: tp / (tp + fn)\n",
    "Predict most postive or churn user correctly. High recall means low fn, not many churn users were predicted as return users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-tP94iFi0OI"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def cal_evaluation(classifier, cm):\n",
    "    tn = cm[0][0]\n",
    "    fp = cm[0][1]\n",
    "    fn = cm[1][0]\n",
    "    tp = cm[1][1]\n",
    "    accuracy  = (tp + tn) / (tp + fp + fn + tn + 0.0)\n",
    "    precision = tp / (tp + fp + 0.0)\n",
    "    recall = tp / (tp + fn + 0.0)\n",
    "    print (classifier)\n",
    "    print (\"Accuracy is: %0.3f\" % accuracy)\n",
    "    print (\"precision is: %0.3f\" % precision)\n",
    "    print (\"recall is: %0.3f\" % recall)\n",
    "\n",
    "def draw_confusion_matrices(confusion_matricies):\n",
    "    class_names = ['Not','Churn']\n",
    "    for cm in confusion_matrices:\n",
    "        classifier, cm = cm[0], cm[1]\n",
    "        cal_evaluation(classifier, cm)\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap('Reds'))\n",
    "        plt.title('Confusion matrix for %s' % classifier)\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticklabels([''] + class_names)\n",
    "        ax.set_yticklabels([''] + class_names)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpSGaN49i0OL"
   },
   "outputs": [],
   "source": [
    "confusion_matrices = [\n",
    "    (\"Random Forest\", confusion_matrix(y_test,best_RF_model.predict(X_test))),\n",
    "    (\"Logistic Regression\", confusion_matrix(y_test,best_LR_model.predict(X_test))),\n",
    "]\n",
    "\n",
    "draw_confusion_matrices(confusion_matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvHlyhPBi0OT"
   },
   "source": [
    "### Part 3.4: Model Evaluation - ROC & AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx_3XkgKi0OW"
   },
   "source": [
    "RandomForestClassifier, KNeighborsClassifier and LogisticRegression have predict_prob() function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Os_ZLTvi0OX"
   },
   "source": [
    "#### Part 3.4.1: ROC of RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UypvQMVBi0OY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn import metrics\n",
    "\n",
    "y_pred_rf = best_RF_model.predict_proba(X_test)[:, 1]\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3PR-PdPi0Ob"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve - RF model')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R89IUMYDi0Oe"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.auc(fpr_rf,tpr_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1DVqnJVi0Oh"
   },
   "source": [
    "#### Part 3.4.2: ROC of LR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-q5XJPoi0Oi"
   },
   "outputs": [],
   "source": [
    "y_pred_lr = best_LR_model.predict_proba(X_test)[:, 1]\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZSrN-1Mi0Ok"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_lr, tpr_lr, label='LR')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve - LR Model')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHAyxishi0On"
   },
   "outputs": [],
   "source": [
    "metrics.auc(fpr_lr,tpr_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHHurD8Ii0Oq"
   },
   "source": [
    "# Part 4: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSx4TPO-i0Or"
   },
   "source": [
    "### Part 4.1:  Logistic Regression Model - Feature Selection Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtLHUixoi0Ot"
   },
   "source": [
    "The corelated features that we are interested in: (total_day_minutes, total_day_charge), (total_eve_minutes, total_eve_charge), (total_intl_minutes, total_intl_charge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQaXOIsUi0Ou",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_l1 = scaler.fit_transform(X)\n",
    "LRmodel_l1 = LogisticRegression(penalty=\"l1\", C = 0.1, solver='liblinear')\n",
    "LRmodel_l1.fit(X_l1, y)\n",
    "LRmodel_l1.coef_[0]\n",
    "print (\"Logistic Regression (L1) Coefficients\")\n",
    "for k,v in sorted(zip(map(lambda x: round(x, 4), LRmodel_l1.coef_[0]), \\\n",
    "                      churn_feat_space.columns), key=lambda k_v:(-abs(k_v[0]),k_v[1])):\n",
    "    print (v + \": \" + str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "majifZZqi0O9"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_l2 = scaler.fit_transform(X)\n",
    "LRmodel_l2 = LogisticRegression(penalty=\"l2\", C = 0.1)\n",
    "LRmodel_l2.fit(X_l2, y)\n",
    "LRmodel_l2.coef_[0]\n",
    "print (\"Logistic Regression (L2) Coefficients\")\n",
    "for k,v in sorted(zip(map(lambda x: round(x, 4), LRmodel_l2.coef_[0]), \\\n",
    "                      churn_feat_space.columns), key=lambda k_v:(-abs(k_v[0]),k_v[1])):\n",
    "    print (v + \": \" + str(k))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqs41ydLi0O_"
   },
   "source": [
    "### Part 4.2:  Random Forest Model - Feature Importance Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPxUM2lei0PA"
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier()\n",
    "forest.fit(X, y)\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "print(\"Feature importance ranking by Random Forest Model:\")\n",
    "for k,v in sorted(zip(map(lambda x: round(x, 4), importances), churn_feat_space.columns), reverse=True):\n",
    "    print (v + \": \" + str(k))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Customer_churn_inclass.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
