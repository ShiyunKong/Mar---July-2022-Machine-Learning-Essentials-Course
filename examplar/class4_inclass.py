# -*- coding: utf-8 -*-
"""Class4_inclass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z2fXvM-R08DUVQvphI9izX76MrXxZRmC

## Linear regression
* using the first feature of diabetes dataset in sk-learn datasets
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score

# load the dataset
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)

diabetes_X[:, np.newaxis, 2].shape

# we only want to use column3(index2) as the predictor
diabetes_X = diabetes_X[:, np.newaxis, 2]

diabetes_X.shape, diabetes_y.shape

# creating the train/test
# we are using the last 20 samples as the test
diabetes_X_train = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]

# do the same thing on y
diabetes_y_train = diabetes_y[:-20]
diabetes_y_test = diabetes_y[-20:]

# creating an empty linear regression object
regr = linear_model.LinearRegression(
    # by entering nothing in the (), you are using the defaut parameters of the function
    # you can also add parameter by yourself
    # you can also add it later, after you declare this object, set_param
)

# fitting this linear regresion to the train_X and train_y
regr.fit(diabetes_X_train, diabetes_y_train)

# beta1
print(regr.coef_[0])
# beta0
print(regr.intercept_)

# making a prediction on the test_X set
diabetes_y_pred = regr.predict(diabetes_X_test)

# The coefficients
print("Coefficients: \n", regr.coef_[0])
# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(diabetes_y_test, diabetes_y_pred))
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %.2f" % r2_score(diabetes_y_test, diabetes_y_pred))

plt.scatter(diabetes_X_test, diabetes_y_test, color='black', label='true points')
plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', label='regression')
plt.xlabel('x value')
plt.ylabel('y value')
plt.title('prediction of our linear regression')
plt.legend()
plt.show()

"""## logistic regression"""

from time import time
import numpy as np
import random
import matplotlib.pyplot as plt

from sklearn import linear_model
from sklearn import datasets
from sklearn.svm import l1_min_c
from sklearn.utils import shuffle

iris = datasets.load_iris()
X = iris.data
y = iris.target
X.shape,y

X = X[y != 2]
y = y[y != 2]
X.shape,y

# standardization/normalization, which can speed up the convergence
X /= X.max()

# do a random shuffle to mix up 0 and 1
X,y = shuffle(X,y)

# train_test_split
train_X = X[:80,]
test_X = X[80:,]

train_y = y[:80,]
test_y = y[80:,]

train_X.shape, train_y.shape, test_X.shape, test_y.shape

# creating an empty LR object
clf = linear_model.LogisticRegression(penalty="l2")

# fit the model
clf.fit(train_X, train_y)

# making the prediction
y_pred = clf.predict(test_X)

# accuracy of the prediction
np.mean(test_y == y_pred)
y_pred

# get the predicted probability of each instance/sample
clf.predict_proba(test_X)

"""## Task: Do the linear regression using the first variable in the iris data set to predict the iris target, plot your prediction line and test sets, verify that we cannot use linear regression to fit the model."""



"""## Random Forest Example"""

import matplotlib.pyplot as plt

from collections import OrderedDict
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier

RANDOM_STATE = 123

X, y = make_classification(
    n_samples=500,
    n_features=25,
    n_clusters_per_class=1,
    n_informative=15,
    random_state=RANDOM_STATE,
)

ensemble_clfs = [
    (
        "RandomForestClassifier, max_features='sqrt'",
        RandomForestClassifier(
            warm_start=True,
            oob_score=True,
            max_features="sqrt",
            random_state=RANDOM_STATE,
        ),
    ),
    (
        "RandomForestClassifier, max_features='log2'",
        RandomForestClassifier(
            warm_start=True,
            max_features="log2",
            oob_score=True,
            random_state=RANDOM_STATE,
        ),
    ),
    (
        "RandomForestClassifier, max_features=None",
        RandomForestClassifier(
            warm_start=True,
            max_features=None,
            oob_score=True,
            random_state=RANDOM_STATE,
        ),
    ),
]

error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)

min_estimators = 15
max_estimators = 150

for label, clf in ensemble_clfs:
    for i in range(min_estimators, max_estimators + 1, 5):
        clf.set_params(n_estimators=i)
        clf.fit(X, y)

        oob_error = 1 - clf.oob_score_
        error_rate[label].append((i, oob_error))

for label, clf_err in error_rate.items():
    xs, ys = zip(*clf_err)
    plt.plot(xs, ys, label=label)

plt.xlim(min_estimators, max_estimators)
plt.xlabel("# of trees")
plt.ylabel("OOB error rate")
plt.legend(loc="upper right")
plt.show()

"""## Recognizing hand-written digits using SVM"""

import matplotlib.pyplot as plt
from sklearn import datasets, svm, metrics
from sklearn.model_selection import train_test_split

digits = datasets.load_digits()

_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))
for ax, image, label in zip(axes, digits.images, digits.target):
    ax.set_axis_off()
    ax.imshow(image, cmap=plt.cm.gray_r, interpolation="nearest")
    ax.set_title("Training: %i" % label)

n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

clf = svm.SVC(gamma=0.001)

X_train, X_test, y_train, y_test = train_test_split(
    data, digits.target, test_size=0.5, shuffle=False
)

clf.fit(X_train, y_train)
predicted = clf.predict(X_test)

np.mean(predicted == y_test)

_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))
for ax, image, prediction, true in zip(axes, X_test, predicted, y_test):
    ax.set_axis_off()
    image = image.reshape(8, 8)
    ax.imshow(image, cmap=plt.cm.gray_r, interpolation="nearest")
    ax.set_title(f"Prediction/true: {prediction, true}")

disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test, predicted)
disp.figure_.suptitle("Confusion Matrix")
print(f"Confusion matrix:\n{disp.confusion_matrix}")

plt.show()

"""## K-means example"""

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from sklearn.cluster import KMeans
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data
y = iris.target

estimators = [
    ("k_means_iris_8", KMeans(n_clusters=8)),
    ("k_means_iris_3", KMeans(n_clusters=3)),
    ("k_means_iris_bad_init", KMeans(n_clusters=3, n_init=1, init="random")),
]

fignum = 1
titles = ["8 clusters", "3 clusters", "3 clusters, bad initialization"]
for name, est in estimators:
    fig = plt.figure(fignum, figsize=(4, 3))
    ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)
    est.fit(X)
    labels = est.labels_

    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(float), edgecolor="k")

    ax.w_xaxis.set_ticklabels([])
    ax.w_yaxis.set_ticklabels([])
    ax.w_zaxis.set_ticklabels([])
    ax.set_xlabel("Petal width")
    ax.set_ylabel("Sepal length")
    ax.set_zlabel("Petal length")
    ax.set_title(titles[fignum - 1])
    ax.dist = 12
    fignum = fignum + 1

"""## PCA"""

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


from sklearn import decomposition
from sklearn import datasets

np.random.seed(5)

iris = datasets.load_iris()
X = iris.data
y = iris.target

X.shape

pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)

X.shape

fig = plt.figure(1, figsize=(4, 3))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)
plt.cla()
for name, label in [("Setosa", 0), ("Versicolour", 1), ("Virginica", 2)]:
    ax.text3D(
        X[y == label, 0].mean(),
        X[y == label, 1].mean() + 1.5,
        X[y == label, 2].mean(),
        name,
        horizontalalignment="center",
        bbox=dict(alpha=0.5, edgecolor="w", facecolor="w"),
    )
# Reorder the labels to have colors matching the cluster results
y = np.choose(y, [1, 2, 0]).astype(float)
ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor="k")

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])

plt.show()

iris = datasets.load_iris()
X = iris.data
y = iris.target
pca = decomposition.PCA(n_components=2)
pca.fit(X)
X = pca.transform(X)

X.shape

fig = plt.figure(1, figsize=(10, 5))
# plt.clf()
# ax = Axes3D(fig, rect=[0, 0, 0.95, 1], elev=48, azim=134)
# plt.cla()
for name, label in [("Setosa", 0), ("Versicolour", 1), ("Virginica", 2)]:
    plt.scatter(X[y==label, 0], X[y==label, 1], label=name)
# Reorder the labels to have colors matching the cluster results
# y = np.choose(y, [1, 2, 0]).astype(float)
plt.legend()
# ax.w_xaxis.set_ticklabels([])
# ax.w_yaxis.set_ticklabels([])
# ax.w_zaxis.set_ticklabels([])

plt.show()

"""variable selection: all variable in new space is in old space

PCA: all variable in the new space is not in old space, but calculated from the old space
"""