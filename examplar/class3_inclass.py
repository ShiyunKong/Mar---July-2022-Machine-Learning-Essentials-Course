# -*- coding: utf-8 -*-
"""Class3_inclass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16eNkJjnIEi0-pIjkDVeGAxak1eXbPmcN

## Requests to a link
"""

import requests

# example: GitHub’s public timeline
r = requests.get('https://api.github.com/events')

# You can find out what encoding Requests is using, and change it
r.encoding
# there are some other encoding, for example 'ISO-8859-1'

# We can read the content of the server’s response. 
r.text

# You can also access the response body as bytes
r.content

# There’s also a builtin JSON decoder
r.json()

from urllib.request import urlopen

url = "http://olympus.realpython.org/profiles/aphrodite"

# urlopen() returns an HTTPResponse object:
page = urlopen(url)

# using read() to return the html body in byte
html_bytes = page.read()
# using decode() to decode the byte to text using utf-8
html = html_bytes.decode("utf-8")
print(html)

"""## string method"""

# title
title_index = html.find('<title>')

# You don’t want the index of the <title> tag, though. You want the index of the title itself.
start_index = title_index + len('<title>')
# Now get the index of the closing </title> tag by passing the string "</title>" to .find()
end_index = html.find('</title>')
start_index, end_index

title = html[start_index:end_index]
title

# another example
# http://olympus.realpython.org/profiles/poseidon
url = "http://olympus.realpython.org/profiles/poseidon"
html = urlopen(url).read().decode("utf-8")
print(html)
start_index = html.find('<title>') + len('<title>')
print(html.find('<title>'))
print(start_index)
end_index = html.find('</title>')
title = html[start_index:end_index]
title

"""## Regular Expressions
Regular expressions are patterns that can be used to search for text within a string. Python supports regular expressions through the standard library’s re module.
"""

import re

"""The regular expression "ab*c" matches any part of the string that begins with an "a", ends with a "c", and has zero or more instances of "b" between the two. re.findall() returns a list of all matches. The string "ac" matches this pattern, so it’s returned in the list."""

# examples
re.findall("ab*c", "ac")
re.findall("ab*c", "abc")
re.findall("ab*c", "abbc")
re.findall("ab*c", "acc")
re.findall("ab*c", "abcac")
re.findall("ab*c", "abdc")
# if no match is found, then findall function will return an empty list
# not reporting error

# Pattern matching is case sensitive. 
# If you want to match this pattern regardless of the case, then you can pass a third argument with the value re.IGNORECASE
re.findall("ab*c", "ABC")
re.findall("ab*c", "ABC", re.IGNORECASE)

"""You can use a period (.) to stand for any single character in a regular expression. For instance, you could find all the strings that contain the letters "a" and "c" separated by a single character as follows:


"""

# examples
re.findall("a.c", "abc")
re.findall("a.c", "abbc")
re.findall("a.c", "ac")
re.findall("a.c", "acc")

"""The pattern .* inside a regular expression stands for any character repeated any number of times. For instance, "a.*c" can be used to find every substring that starts with "a" and ends with "c", regardless of which letter—or letters—are in between:"""

# examples
re.findall("a.*c", "abc")
re.findall("a.*c", "abbc")
re.findall("a.*c", "acabc")
re.findall("a.*c", "acc")

"""* Often, you use re.search() to search for a particular pattern inside a string. This function is somewhat more complicated than re.findall() because it returns an object called a MatchObject that stores different groups of data. This is because there might be matches inside other matches, and re.search() returns every possible result.
* re.search() function will search the regular expression pattern and return the first occurrence. 
* The details of the MatchObject are irrelevant here. For now, just know that calling .group() on a MatchObject will return the first and most inclusive result, which in most cases is just what you want:
"""

match_results = re.search("ab*c", "ABCabbbc", re.IGNORECASE)
# return the tuple of the first occurance index
match_results.span()

# return the string passed into the function
match_results.string

# return the matched string, match_results.string[match_results.span()[0]:match_results.span()[1]]
match_results.group()

"""re.sub(), which is short for substitute, allows you to replace text in a string that matches a regular expression with new text."""

string = "Everything is <replaced> if it's in <tags>."
re.sub('<.*>', 'ELEPHANTS', string)

"""* re.sub() uses the regular expression "<.*>" to find and replace everything between the first < and last >, which spans from the beginning of <replaced> to the end of <tags>. This is because Python’s regular expressions are greedy, meaning they try to find the longest possible match when characters like * are used. (greedy search)
* Alternatively, you can use the non-greedy matching pattern *?, which works the same way as * except that it matches the shortest possible string of text:


"""

string = "Everything is <replaced> if it's in <tags>."
re.sub('<.*?>', 'ELEPHANTS', string)

"""### Extract Text From HTML With Regular Expressions"""

url = "http://olympus.realpython.org/profiles/dionysus"
page = urlopen(url)
html = page.read().decode("utf-8")

pattern = "<title.*?>.*?</title.*?>"
match_results = re.search(pattern, html, re.IGNORECASE)
title = match_results.group()
print(title)
title = re.sub("<.*?>", "", title)
print(title)

"""## Exercise: Scrape Data From a Website [http://olympus.realpython.org/profiles/dionysus]

find the Name and Favorite Color
"""

url = 'http://olympus.realpython.org/profiles/dionysus'
html_page = urlopen(url)
html_text = html_page.read().decode("utf-8")
# 'Name:', 'Favorite Color:'
string = 'Name:'
target = {'Name:':[],
          'Favorite Color:':[]}
for string in ['Name:', 'Favorite Color:']:
    # print(html_text)
    string_start_idx = html_text.find(string)
    text_start_idx = string_start_idx + len(string)

    remain_html = html_text[text_start_idx:]
    ending_idx = remain_html.find('<')
    # print(remain_html)
    text_end_idx = ending_idx + text_start_idx
    raw_text = html_text[text_start_idx:text_end_idx]
    clean_text = raw_text.strip(' \r\n\t')
    print(clean_text)
    target[string] = clean_text
print(target)

'''
\r remove the current line and start over
\n new line
\t tab
'''

a = 'happy\tday'
print(a)

"""## Use an HTML Parser for Web Scraping in Python: BeautifulSoup

"""

# !pip3 install bs4/BeautifulSoup
from bs4 import BeautifulSoup
from urllib.request import urlopen

url = "http://olympus.realpython.org/profiles/dionysus"
page = urlopen(url)
html = page.read().decode("utf-8")
soup = BeautifulSoup(html, "html.parser")

print(soup.get_text())
print(soup.prettify())

# some simple ways to navigate that data structure
soup.title.name
soup.title.string
soup.title.parent.name

# find all apperance of 'img'
soup.find_all('img')
image1, image2 = soup.find_all('img')
image1.name
image1['src'], image2['src']

soup.find_all('img', src='/static/dionysus.jpg')[0]

# find the tag
soup.img
src = []
for i in soup.find_all('img'):
    src.append(i['src'])
    print(i['src'])

"""## Exercise: 
write program that grabs the full html from the page: http://olympus.realpython.org/profiles

The output should look like this:
* http://olympus.realpython.org/profiles/aphrodite
* http://olympus.realpython.org/profiles/poseidon
* http://olympus.realpython.org/profiles/dionysus
"""

base_url = 'http://olympus.realpython.org'
page = urlopen(base_url+'/profiles')
html = page.read().decode("utf-8")
soup = BeautifulSoup(html, "html.parser")
all_link_url = []
for link in soup.find_all('a'):
    link_url = base_url + link['href']
    all_link_url.append(link_url)
    print(link_url)
all_link_url

"""## BeautifulSoup Continued

Beautiful Soup transforms a complex HTML document into a complex tree of Python objects.
"""

# tag
soup = BeautifulSoup('<b class="boldest">Extremely bold</b>', 'html.parser')
tag = soup.b
tag

# name
tag.name
# you can change a tag’s name
tag.name = 'quotation'
tag

# A tag may have any number of attributes. The tag <b id="boldest"> has an attribute “id” whose value is “boldest”. 
tag = BeautifulSoup('<b id="boldest">bold</b>', 'html.parser').b
tag['id']

# see all attributes
tag.attrs

# change/add attributes
tag['id'] = 'verybold'
tag['another_attribute'] = 1
tag

# delete an attribute
del tag['id']
del tag['another_attribute']
tag

# find id?
# tag['id']

# by using .get function, you can avoid getting error message when attr doesn't exist
print(tag.get('id'))

# Multi-valued attributes
css_soup = BeautifulSoup('<p class="body"></p>', 'html.parser')
css_soup.p['class']

css_soup = BeautifulSoup('<p class="body strikeout"></p>', 'html.parser')
css_soup.p['class']

"""If an attribute looks like it has more than one value, but it’s not a multi-valued attribute as defined by any version of the HTML standard, Beautiful Soup will leave the attribute alone"""

id_soup = BeautifulSoup('<p id="my id"></p>')
id_soup.p['id']
# ['my', 'id']

# When you turn a tag back into a string, multiple attribute values are consolidated:
rel_soup = BeautifulSoup('<p>Back to the <a rel="index">homepage</a></p>')
print(rel_soup.a['rel'])
rel_soup.a['rel'] = ['index', 'contents']
print(rel_soup.p)

# If you parse a document as XML, there are no multi-valued attributes:
xml_soup = BeautifulSoup('<p class="body strikeout"></p>', 'xml')
xml_soup.p['class']

# searching the tree
html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>

<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""

from bs4 import BeautifulSoup
soup = BeautifulSoup(html_doc, 'html.parser')

# find all tag b
soup.find_all('b')[0]

# finding all tag starts with b
import re
for tag in soup.find_all(re.compile("^b")):
    print(tag.name)

# finding all tag with/contains t
for tag in soup.find_all(re.compile("t")):
    print(tag.name)

# pass in a list to the find_all function, 
# it is going to find all matches of each element
soup.find_all(["a", "b"])

soup.find_all("a")

soup.find_all("p")
# soup.find_all('p', 'title')
# soup.find_all('p', class_='title')

# you do not have to include the tag name in the find_all fundtion
# you can put into tagname/any attributes you want into the find_all function
soup.find_all(id='link2')

# find, find_all function, you can specify the section you want to search on
soup.find(string=re.compile("sisters"))
soup.find_all(href=re.compile("elsie"))

# find all tag that has the attr of id
soup.find_all(id=True)

"""## Practice:
* send a request to bilibili
* using html parser
* find all item with name div and class_='item', style=False
* print the link in href is exists
* save the links into a list
"""

# https://www.bilibili.com/

import requests
from bs4 import BeautifulSoup

link = 'https://www.bilibili.com/'

r = requests.get(link)
r.encoding = 'utf-8'
soup = BeautifulSoup(r.text, "html.parser")
# print(soup.prettify())

# finding the parameter/attributes of your target section
tags = soup.find_all('div', class_='item', style=False)

# tag.get
links = []
for tag in tags:
    try:
        if str(tag.a['href'])!='javascript:;':
            links.append(str(tag.a['href']))
    except:
        pass

links